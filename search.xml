<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>hbase的安装</title>
      <link href="posts/7018.html"/>
      <url>posts/7018.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>这里要特别感谢ysgg大佬分享他的踩坑记录，感谢他！</p><p>强烈推荐<a href="http://dblab.xmu.edu.cn/blog/install-hbase/">林子雨老师的博客</a>，内容非常详细</p><p>此篇笔记是我在不知道他的博客情况下写的，因此与他的有些不同，但亲测可行，如果遇到版本问题可以再查看我的笔记，或者在评论区提问</p></blockquote><h2 id="一、下载安装"><a href="#一、下载安装" class="headerlink" title="一、下载安装"></a>一、下载安装</h2><p>==安装使用hbase首先需要安装zookeeper（也可用内置的zookeeper）==</p><p><a href="http://hbase.apache.org/downloads.html">传送门</a></p><p>这里是hbase2.3.2</p><p>依然解压后放在<code>/usr/local</code>下，并重新命名为<code>hbase</code>(可选)</p><blockquote><p>依然在hadoop用户下操作</p></blockquote><h2 id="二、配置文件"><a href="#二、配置文件" class="headerlink" title="二、配置文件"></a>二、配置文件</h2><h3 id="1-配置环境变量"><a href="#1-配置环境变量" class="headerlink" title="1. 配置环境变量"></a>1. 配置环境变量</h3><p>在<code>/home/hadoop/.bashrc</code>文件后添加</p><pre><code>export HBASE_HOME=/usr/local/hbaseexport PATH=$PATH:$HBASE_HOME/binexport HBASE_LOG_DIR=/usr/local/hbase/logs</code></pre><p>配置完后记得运行命令</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">source</span> /home/hadoop/.bashrc<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="2-配置其他文件"><a href="#2-配置其他文件" class="headerlink" title="2. 配置其他文件"></a>2. 配置其他文件</h3><h4 id="2-1-hbase-env-sh"><a href="#2-1-hbase-env-sh" class="headerlink" title="2.1 hbase-env.sh"></a>2.1 hbase-env.sh</h4><p>修改<code>hbase/conf</code>文件下的配置文件<code>hbase-env.sh</code></p><pre class="line-numbers language-bash"><code class="language-bash">vim hbase-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre><code>export JAVA_HOME=/usr/local/jdk11export HBASE_MANAGES_ZK=false  #使用外部zookeeper，所以改成false</code></pre><h4 id="2-2-hbase-site-xml"><a href="#2-2-hbase-site-xml" class="headerlink" title="2.2 hbase-site.xml"></a>2.2 hbase-site.xml</h4><p>修改里面的内容</p><pre><code>  &lt;property&gt;    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.tmp.dir&lt;/name&gt;    &lt;value&gt;./tmp&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt;    &lt;value&gt;false&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.rootdir&lt;/name&gt;    &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;    &lt;value&gt;localhost&lt;/value&gt;  &lt;/property&gt;  &lt;/property&gt;   &lt;property&gt;    &lt;name&gt;hbase.master.info.port&lt;/name&gt;    &lt;value&gt;16010&lt;/value&gt;  &lt;/property&gt; &lt;property&gt;    &lt;name&gt;hbase.wal.provider&lt;/name&gt;    &lt;value&gt;filesystem&lt;/value&gt;  &lt;/property&gt;</code></pre><h2 id="三、开启服务"><a href="#三、开启服务" class="headerlink" title="三、开启服务"></a>三、开启服务</h2><p>对于hbase启动顺序：zookeeper–&gt;hadoop–&gt;hbase</p><p>对于hbase关闭顺序：hbase–&gt;hadoop–&gt;zookeeper</p><p>运行以下命令：(注意是在hadoop普通用户下运行)</p><pre><code>sudo service ssh startzkServer.sh start zoo1.cfgzkServer.sh start zoo2.cfgzkServer.sh start zoo3.cfgstart-all.shstart-hbase.shhbase shell</code></pre><p>结果应该是出现<code>xxx&gt;</code>，可在后面输入命令</p><p>输入<code>list</code>，<kbd>enter</kbd>。如果不报错则安装成功</p><h2 id="四、可能的问题"><a href="#四、可能的问题" class="headerlink" title="四、可能的问题"></a>四、可能的问题</h2><h3 id="1-没有master节点error-KeeperErrorCode-NoNode-for-hbase-master"><a href="#1-没有master节点error-KeeperErrorCode-NoNode-for-hbase-master" class="headerlink" title="1. 没有master节点error: KeeperErrorCode = NoNode for /hbase/master"></a>1. 没有master节点error: KeeperErrorCode = NoNode for /hbase/master</h3><p>重启<code>zookeeper</code>，<code>hadoop</code>，zookeeper状态必须保证是<code>STARTED</code></p><blockquote><p>反正出现该问题，不是hbase本身问题，而是zookeeper的影响，因为zookeeper是监控整个hbase节点状态，控制hbase集群通信的重要工具。</p></blockquote><h3 id="2-Call-to-localhost-127-0-0-1-9000-failed-on-connection-exception错误"><a href="#2-Call-to-localhost-127-0-0-1-9000-failed-on-connection-exception错误" class="headerlink" title="2. Call to localhost/127.0.0.1:9000 failed on connection exception错误"></a>2. Call to localhost/127.0.0.1:9000 failed on connection exception错误</h3><p>将目录 <code>/usr/local/hadoop/tmp/dfs/name</code>和 <code>/usr/local/hadoop/tmp/dfs/data</code>中的内容清空</p><p>执行</p><pre class="line-numbers language-bash"><code class="language-bash">bin/hadoop namenode -format<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>重启hadoop</p><h3 id="…持续更新其他问题"><a href="#…持续更新其他问题" class="headerlink" title="…持续更新其他问题"></a>…持续更新其他问题</h3>]]></content>
      
      
      <categories>
          
          <category> hbase </category>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> zookeeper </tag>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zookeeper的安装</title>
      <link href="posts/46537.html"/>
      <url>posts/46537.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>这里要特别感谢ysgg大佬分享他的踩坑记录，感谢他！</p></blockquote><h2 id="一、下载zookeeper"><a href="#一、下载zookeeper" class="headerlink" title="一、下载zookeeper"></a>一、下载zookeeper</h2><p><a href="https://webscripts.softpedia.com/script/Development-Scripts-js/Complete-applications/Apache-Hadoop-ZooKeeper-59851.html">传送门</a></p><p>这里的版本是3.6.2</p><p>下载好后解压放在<code>/usr/local</code>下，并且重命名为zookeeper(不是必须)</p><blockquote><p>依然接着上一篇文章说的，在hadoop用户下操作</p></blockquote><h2 id="二、配置文件"><a href="#二、配置文件" class="headerlink" title="二、配置文件"></a>二、配置文件</h2><p>在zookeeper目录下创建文件夹<code>dataDir</code>、<code>dataLogDir</code>和bin同级</p><p>dataDir下创建三个文件夹<code>zoo1_datadir</code>、<code>zoo2_datadir</code>、<code>zoo3_datadir</code>,并且在每个文件夹中创建一个名为<code>myid</code>的文件，注意没有后缀名，内容分别为1，2，3即可</p><p>dataLogDir下创建三个目录<code>zoo1_datalogdir</code>、<code>zoo2_datalogdir</code>、<code>zoo3_datalogdir</code></p><p>在conf目录下根据zoo_template.cfg复制三个文件</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">cp</span> -r zoo_template.cfg zoo1.cfg<span class="token function">cp</span> -r zoo_template.cfg zoo2.cfg<span class="token function">cp</span> -r zoo_template.cfg zoo3.cfg<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>其中内容粘贴以下内容：</p><pre><code># The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes.#这里是要配置的缓存和日志地址dataDir=/usr/local/zookeeper/dataDir/zoo1_datadirdataLogDir=/usr/local/zookeeper/dataLogDir/zoo1_datalogdir# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1## Metrics Providers## https://prometheus.io Metrics Exporter#metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider#metricsProvider.httpPort=7000#metricsProvider.exportJvmInfo=true#这里的server.n中的n为myid里的值server.1=localhost:2887:3887server.2=localhost:2888:3888server.3=localhost:2889:3889</code></pre><p>==注意==:<br>每一个文件的内容需要该的地方是</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#这里是要配置的缓存和日志地址</span>dataDir<span class="token operator">=</span>/usr/local/zookeeper/dataDir/zoo1_datadirdataLogDir<span class="token operator">=</span>/usr/local/zookeeper/dataLogDir/zoo1_datalogdir<span class="token comment" spellcheck="true"># the port at which the clients will connect</span>clientPort<span class="token operator">=</span>2181<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>前两个不用多说，最后一个端口号分别为2181、2182、2183</p><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p>在<code>/home/hadoop/.bashrc</code>文件后添加以下内容</p><pre><code>export ZOOKEEPER_HOME=/usr/local/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/binexport ZOO_LOG_DIR=/usr/local/zookeeper/logs</code></pre><p>配置完后记得运行命令</p><pre class="line-numbers language-bash"><code class="language-bash"><span class="token function">source</span> /home/hadoop/.bashrc<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>此时配置已经完成</p><h2 id="三、启动"><a href="#三、启动" class="headerlink" title="三、启动"></a>三、启动</h2><p>运行以下命令以测试是否开启成功</p><pre class="line-numbers language-bash"><code class="language-bash">zkServer.sh start zoo1.cfgzkServer.sh start zoo2.cfgzkServer.sh start zoo3.cfg<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>结果应为<code>STARTED</code></p><p>运行以下命令以查看三个节点的状态</p><pre class="line-numbers language-bash"><code class="language-bash">zkServer.sh status zoo1.cfgzkServer.sh status zoo2.cfgzkServer.sh status zoo3.cfg<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>结果应为<code>follwer</code>之类的主从关系</p><h2 id="四、可能出现的问题"><a href="#四、可能出现的问题" class="headerlink" title="四、可能出现的问题"></a>四、可能出现的问题</h2><h3 id="1-开启成功，检查状态的时候发现不是主从关系"><a href="#1-开启成功，检查状态的时候发现不是主从关系" class="headerlink" title="1. 开启成功，检查状态的时候发现不是主从关系"></a>1. 开启成功，检查状态的时候发现不是主从关系</h3><p>检查配置文件是否出错</p><p>重新启动zookeeper</p><p>如果还是有问题，说明zookeeper未重启成功，删除zk数据目录下之前生成的<code>version-2</code>文件夹，再重启。</p><h3 id="2-开启成功，检查状态发现开启失败"><a href="#2-开启成功，检查状态发现开启失败" class="headerlink" title="2. 开启成功，检查状态发现开启失败"></a>2. 开启成功，检查状态发现开启失败</h3><p>检查配置文件是否出错</p><p>重新启动zookeeper</p><p>如果还是有问题，说明zookeeper未重启成功，删除zk数据目录下之前生成的<code>version-2</code>文件夹，再重启。</p><h3 id="3-开启失败"><a href="#3-开启失败" class="headerlink" title="3. 开启失败"></a>3. 开启失败</h3><p>配置文件出错</p>]]></content>
      
      
      <categories>
          
          <category> hbase </category>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> 大数据 </tag>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop的安装(Ubantu、伪分布式)</title>
      <link href="posts/7584.html"/>
      <url>posts/7584.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>这里要特别感谢ysgg大佬分享他的踩坑记录，感谢他！</p><p>强烈推荐<a href="http://dblab.xmu.edu.cn/blog/2775-2/">林子雨老师的博客</a>，内容非常详细</p><p>此篇笔记是我在不知道他的博客情况下写的，因此与他的有些不同，但亲测可行，如果遇到版本问题可以再查看我的笔记，或者在评论区提问</p></blockquote><h2 id="一、下载jdk配置环境变量"><a href="#一、下载jdk配置环境变量" class="headerlink" title="一、下载jdk配置环境变量"></a>一、下载jdk配置环境变量</h2><h3 id="1-下载jdk"><a href="#1-下载jdk" class="headerlink" title="1. 下载jdk"></a>1. 下载jdk</h3><p>官网下载jdk，<a href="https://www.oracle.com/cn/java/technologies/javase-downloads.html">传送门</a></p><p>这里使用的是jdk11</p><h3 id="2-配置环变量"><a href="#2-配置环变量" class="headerlink" title="2. 配置环变量"></a>2. 配置环变量</h3><p>个人为了不影响本用户，这里新创建一个<code>hadoop</code>用户，==后面所有操作均在hadoop用下完成，包括文件的用户和组均为hadoop==</p><p>将环变量配置到<code>/home/hadoop/.bashrc</code></p><p><code>.bashrc</code>是用户环境变量，<code>/etc/profile</code>是全局环境变量</p><pre class="line-numbers language-bash"><code class="language-bash">vim /home/hadoop/.bashrc<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>添加以下</p><pre><code>export JAVA_HOME=xxx #这里我是/usr/local/jdk11export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$JAVA_HOME/bin:$PATH</code></pre><h2 id="二、下载软件包并安装"><a href="#二、下载软件包并安装" class="headerlink" title="二、下载软件包并安装"></a>二、下载软件包并安装</h2><p>去官网下载，<a href="https://hadoop.apache.org/releases.html">传送门</a>。</p><p>这里使用的是<code>hadoop3.3.0</code></p><p>下载之后解压，将解压后的内容移动到<code>/usr/local</code>下</p><p>并重命名为<code>hadoop</code>（可选）</p><h2 id="三、安装ssh-设置无密码登陆"><a href="#三、安装ssh-设置无密码登陆" class="headerlink" title="三、安装ssh 设置无密码登陆"></a>三、安装ssh 设置无密码登陆</h2><blockquote><p>对于Hadoop的伪分布式和全分布式而言，hadoop名称节点需要启动集群中所有机器的hadoop守护进程，这个过程可通过ssh登录来实现。</p></blockquote><p>安装ssh serer,并启动服务</p><pre class="line-numbers language-bash"><code class="language-bash">$ <span class="token function">sudo</span> apt <span class="token function">install</span> openssh-server       <span class="token comment" spellcheck="true">#安装SSH server</span>$ <span class="token function">ps</span> -e <span class="token operator">|</span> <span class="token function">grep</span> <span class="token function">ssh</span>                      <span class="token comment" spellcheck="true">#可以看到进程sshd或者使用命令：ssh -V， 查看ssh版本</span>$ <span class="token function">sudo</span> <span class="token function">service</span> <span class="token function">ssh</span> start<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>尝试登陆看是否成功，此时要输入密码，后面配置好ssh免密登录后即可不需要输入密码</p><pre class="line-numbers language-bash"><code class="language-bash">$ <span class="token function">ssh</span> localhost                         <span class="token comment" spellcheck="true">#登陆SSH，第一次登陆输入yes</span>$ <span class="token keyword">exit</span>                                  <span class="token comment" spellcheck="true">#退出登录的ssh localhost</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>开始配置免密登录</p><p>配置sshd_config</p><pre class="line-numbers language-bash"><code class="language-bash">$ <span class="token function">cd</span> /etc/ssh$ <span class="token function">sudo</span> vim sshd_config<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>替换以下内容</p><pre><code># $OpenBSD: sshd_config,v 2.93 2014/01/10 05:59:19 djm Exp $# This is the sshd server system-wide configuration file.  See# sshd_config(5) for more information.# This sshd was compiled with PATH=/usr/local/bin:/usr/bin# The strategy used for options in the default sshd_config shipped with# OpenSSH is to specify options with their default value where# possible, but leave them commented.  Uncommented options override the# default value.# If you want to change the port on a SELinux system, you have to tell# SELinux about this change.# semanage port -a -t ssh_port_t -p tcp #PORTNUMBER##Port 22#AddressFamily any#ListenAddress 0.0.0.0#ListenAddress ::# The default requires explicit activation of protocol 1#Protocol 2# HostKey for protocol version 1#HostKey /etc/ssh/ssh_host_key# HostKeys for protocol version 2HostKey /etc/ssh/ssh_host_rsa_keyHostKey /etc/ssh/ssh_host_dsa_key#HostKey /etc/ssh/ssh_host_dsa_key#HostKey /etc/ssh/ssh_host_ecdsa_key#HostKey /etc/ssh/ssh_host_ed25519_key# Lifetime and size of ephemeral version 1 server key#KeyRegenerationInterval 1h#ServerKeyBits 1024# Ciphers and keying#RekeyLimit default none#ServerKeyBits 1024# Ciphers and keying#RekeyLimit default none# Logging# obsoletes QuietMode and FascistLogging#SyslogFacility AUTHSyslogFacility AUTHPRIV#LogLevel INFO# Authentication:#LoginGraceTime 2mPermitRootLogin yes#StrictModes yes#MaxAuthTries 6#MaxSessions 10RSAAuthentication yesPubkeyAuthentication yes# The default is to check both .ssh/authorized_keys and .ssh/authorized_keys2# but this is overridden so installations will only check .ssh/authorized_keysAuthorizedKeysFile      .ssh/authorized_keys#AuthorizedPrincipalsFile none#AuthorizedKeysCommand none#AuthorizedKeysCommandUser nobody# For this to work you will also need host keys in /etc/ssh/ssh_known_hosts#RhostsRSAAuthentication no# similar for protocol version 2#HostbasedAuthentication no# Change to yes if you don&#39;t trust ~/.ssh/known_hosts for# RhostsRSAAuthentication and HostbasedAuthentication#IgnoreUserKnownHosts no# Don&#39;t read the user&#39;s ~/.rhosts and ~/.shosts files#IgnoreRhosts yes# To disable tunneled clear text passwords, change to no here!#PasswordAuthentication yes#PermitEmptyPasswords no# Change to no to disable s/key passwords#ChallengeResponseAuthentication yesChallengeResponseAuthentication no# Kerberos options#KerberosAuthentication no#KerberosOrLocalPasswd yes#KerberosTicketCleanup yes#KerberosGetAFSToken no#KerberosUseKuserok yes# GSSAPI optionsGSSAPIAuthentication noGSSAPICleanupCredentials no#GSSAPIStrictAcceptorCheck yes#GSSAPIKeyExchange no#GSSAPIEnablek5users no# Set this to &#39;yes&#39; to enable PAM authentication, account processing,# and session processing. If this is enabled, PAM authentication will# be allowed through the ChallengeResponseAuthentication and# PasswordAuthentication.  Depending on your PAM configuration,# PAM authentication via ChallengeResponseAuthentication may bypass# the setting of &quot;PermitRootLogin without-password&quot;.# If you just want the PAM account and session checks to run without# PAM authentication, then enable this but set PasswordAuthentication# and ChallengeResponseAuthentication to &#39;no&#39;.# WARNING: &#39;UsePAM no&#39; is not supported in Red Hat Enterprise Linux and may cause several# problems.UsePAM yes#AllowAgentForwarding yes#AllowTcpForwarding yes#GatewayPorts noX11Forwarding yes#X11DisplayOffset 10#X11UseLocalhost yes#PermitTTY yes#PrintMotd yes#PrintLastLog yes#TCPKeepAlive yes#UseLogin no#UsePrivilegeSeparation yes          # Default for new installations.#PermitUserEnvironment no#Compression delayed#ClientAliveInterval 0#ClientAliveCountMax 3#ShowPatchLevel noUseDNS no#PidFile /var/run/sshd.pid#MaxStartups 10:30:100#PermitTunnel no#ChrootDirectory none#VersionAddendum none# no default banner path#Banner none# Accept locale-related environment variablesAcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGESAcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENTAcceptEnv LC_IDENTIFICATION LC_ALL LANGUAGEAcceptEnv XMODIFIERS# override default of no subsystemsSubsystem       sftp    /usr/libexec/openssh/sftp-server# Example of overriding settings on a per-user basis#Match User anoncvs#       X11Forwarding no#       AllowTcpForwarding no#       PermitTTY no#       ForceCommand cvs server</code></pre><p>ssh设置免密码登陆</p><pre><code>$ cd ~/.ssh/ #如果没法进入该目录，执行一次ssh localhost$ ssh-keygen -t rsa #生成密钥对$ ssh-keygen -t dsa$ cat id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys$ cat id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys #设置免密登陆</code></pre><p>再次登录即可不需要密码</p><blockquote><p>此步如果出现know_host错误可以尝试将当前文件中的know_host文件删除</p></blockquote><h2 id="四、单机安装hadoop"><a href="#四、单机安装hadoop" class="headerlink" title="四、单机安装hadoop"></a>四、单机安装hadoop</h2><h3 id="1-配置环境变量"><a href="#1-配置环境变量" class="headerlink" title="1. 配置环境变量"></a>1. 配置环境变量</h3><p>类似配置jdk，加上</p><pre><code>export HADOOP_HOME=/usr/local/hadoopexport CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath):$CLASSPATHexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport HADOOP_OPTS=&quot;-Djava.library.path=$HADOOP_HOME/lib:$HADOOP_COMMON_LIB_NATIVE_DIR&quot;</code></pre><h3 id="2-单机安装的配置"><a href="#2-单机安装的配置" class="headerlink" title="2. 单机安装的配置"></a>2. 单机安装的配置</h3><p>进入目录 <code>/usr/local/hadoop/etc/hadoop</code>,找到以下文件</p><ul><li>hadoop-env.sh</li><li>yarn-env.sh</li><li>mapred-env.sh</li></ul><p>均要添加<code>export JAVA_HOME=xxxxx</code></p><p>此时使用，应有hadoop版本信息</p><pre class="line-numbers language-bash"><code class="language-bash">hadoop version<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="五、伪分布式安装"><a href="#五、伪分布式安装" class="headerlink" title="五、伪分布式安装"></a>五、伪分布式安装</h2><p>core-site.xml配置</p><pre><code>&lt;configuration&gt;    &lt;property&gt;         &lt;name&gt;fs.defaultFS&lt;/name&gt;         &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;         &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;         &lt;value&gt;/usr/local/hadoop/tmp&lt;/value&gt;    &lt;/property&gt;  &lt;/configuration&gt;</code></pre><p>hdfs-site.xml配置</p><pre><code>&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;dfs.replication&lt;/name&gt;                &lt;value&gt;1&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;                &lt;value&gt;/usr/local/hadoop/tmp/dfs/name&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;                &lt;value&gt;/usr/local/hadoop/tmp/dfs/data&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>mapred-site.xml配置</p><pre><code>&lt;configuration&gt;        &lt;property&gt;                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;                &lt;value&gt;yarn&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>yarn-site.xml配置</p><pre><code>&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt;        &lt;property&gt;                 &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;                 &lt;value&gt;mapreduce_shuffle&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;                  &lt;value&gt;localhost&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;                &lt;name&gt;yarn.application.classpath&lt;/name&gt;                &lt;value&gt;使用`hadoop classpath`命令查询,将结果放在此处&lt;/value&gt;        &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>配置完后初始化一下。否则后面会报错没有<code>namenode</code>节点</p><pre class="line-numbers language-bash"><code class="language-bash">hadoop namenode -format<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>再运行</p><pre class="line-numbers language-bash"><code class="language-bash">start-all.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>即可开启hadoop服务</p><p>如果报错没有<code>start-all.sh</code>命令，即环境变量配置的问题，后面都是如此。</p><p>注意看开启的信息，如果成功输入命令<code>jps</code><br>可得到如下类似结果</p><pre><code>NodeManagerJpsNameNodeSecondaryNameNodeResourceMangerDataNode</code></pre><p>注意是6个</p><p>这个时候可以打开hadoop的web页面 <code>localhost:9870</code></p><p>接下来尝试使用hadoop自带的wordcount来实现单词统计的功能</p><p>创建一个文件<code>test.txt</code>里面内容为<code>hello hadoop my hadoop</code></p><p> 注意以下的test.txt文件路径为我自己的文件路径<br> 依次输入以下命令</p><pre><code>hdfs dfs -mkdir -p /test/input #创建入口hdfs dfs -ls / #查看有哪些文件hdfs dfs -put /home/hadoop/test.txt /test/input #将test.txt放入hdfs dfs -cat /test/input/test.txt  #查看放入的test.txt文件内容yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar wordcount /test/input /test/output  #使用wordcounthdfs dfs -cat /test/output/part-r-00000 #查看结果</code></pre><p> hadoop的简单配置到此结束！</p>]]></content>
      
      
      <categories>
          
          <category> hadoop 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>个人小站创建成功！欢迎来访！</title>
      <link href="posts/41794.html"/>
      <url>posts/41794.html</url>
      
        <content type="html"><![CDATA[<p>经过多番摸索，终于在今天创建好了我的个人小站！ 今后它将与我共同成长，一起进步，一起变得更完美！</p><p>我是刘书程，一个前端小白~</p>]]></content>
      
      
      <categories>
          
          <category> 日记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
